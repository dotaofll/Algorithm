Title         : My Academic Paper

Author        : Yuzhe Liu
Affiliation   : XJ
Email         : dotaofll@163.com

Author        : My co-author
Affiliation   : Other institute
Email         : other@bar.com

Bibliography  : example.bib
Doc class     : [reprint,nocopyrightspace]style/sigplanconf.cls

[TITLE]

~ Abstract
There is more in you of good than you know, child of the kindly West.
Some courage and some wisdom, blended in measure. If more of us valued
food and cheer and song above hoarded gold, it would be a merrier world.
But sad or merry, I must leave it now. Farewell!
~

~ TexRaw
% any commands necessary for your particular style
\category{D.2.5}{Software Engineering}{Testing and Debugging}[symbolic execution]
\terms{Algorithms, Experimentation}
\keywords{Games for learning, white box testing}
\cite{@johnson2017google}
~

# Introduction     { #sec-intro }

&nbsp;&nbsp;In the past four years, neural machine translation has demonstrated excellent performance in many fields.
Moreover, the best results were obtained in the IWSLT(@cettolo2017overview) and WMT(@bojar2016findings) evaluation competition.
Neural machine translation is an end-to-end fashion to model the mapping of source language to target language through posterior probability.
The NMT system consists of an encoder and decoder, which jointly train the maximum likelihood probability in an end-to-end manner.
With the help of long-term and short-term memory neural networks, threshold loop units, attention mechanisms, etc., NMT has achieved good results in many translation tasks, especially the latter.
Among the different variants, two are the most concerned, one is recurrent NMT and the other is transformer NMT models(@sutskever2014sequence,@bahdanau2014neural,@vaswani2017attention).
Although, NMT has achieved state-of-the-art performance on many translation tasks.However, NMT is also plagued by data sparseness.
@zoph2016transfer indicate that NMT obtains much worse translation quality than a statistical machine translation system.
In response to this problem, it is necessary to construct an effective NMT to cope with the situation of low resource languages.


&nbsp;&nbsp;As a result,Many scholars are working hard to explore new methods of translating languages, when faced with insufficient or no parallel corpus.
@firat2016multi present a multi-way,multilingual model with shared attention to achieve zero-resource language pair.Another direction is to develop an universal encoder-decoder architecture.
The simplest and most efficient implementation method is to use target-forcing technique,which is to perpend to the source sentence a tag specifying the target language,both training and testing time(@johnson2017google,@ha2016toward).
Since the transformer model was proposed, many translation tasks have replaced rnn-based NMT with transformer NMT.


&nbsp;&nbsp;Another direction is to achieve an source-to-target NMT without parallel corpus via a pivot.@cheng2016neural proposed a method for zero-resource NMT.
Although these approaches prove to be effective,but pivot-based approaches usually need to divide the decoding process into two steps,which is not only more computationally expensive, but also potentially suffers from the error propagation problem(@zhu2013improving).
@chen2017teacher proposed an NMT based on knowledge distillation and pre-training.They called source-to-pivot NMT "student" and pivot-to-target NMT "teacher" and they pre-train the "teacher" model and use "teacher" model to guide the learning process of the student model on a source-pivot parallel corpus.
Compared with pivot-based approaches(@cheng2016neural),their method directly estimates the posterior probability of the source language to the target language.Therefore this strategy not only improves efficiency but also avoids error propagation in decoding.

&nbsp;&nbsp;In this work,we approach low-resource machine translation with so-called pivot-based NMT(@cheng2016neural,@chen2017teacher).The Transformer approach delivers the best performing multilingual models,with a larger gain over
corresponding bilingual models than observed with RNNs and it delivers the best quality in all considered zero-shot condition and translation directions(@lakew2018comparison).
Our motivation is based on the above two points and the method of @chen2017teacher .So our approach is a pivot-based transformer NMT.Not only reduces the calculation cost, but also avoids the second propagation of error. More importantly, the transformer NMT has been proven to perform better than rnn-based NMT.
Therefore our approach should ideally help in the case of similar languages and sparse training data.Experiments on the Europarl and WMT datasets show
that our approach achieves significant improvements in terms of both translation quality and decoding efficiency over a baseline pivot-based approach to zero-resource NMT on Spanish-French
and German-French translation tasks

# Related Work {#sec-related-work}

## zero-shot learning.
 * try to explain the NMT has attract more people attention.
  The NMT system consists of an encoder and decoder, which jointly train the maximum likelihood probability in an end-to-end manner.
  With the help of long-term and short-term memory neural networks, threshold loop units, attention mechanisms, etc., NMT has achieved good results in many translation tasks, especially the latter.
  Among the different variants, two are the most concerned, one is recurrent NMT and the other is transformer NMT models.
  
 * google's team propose a multilingual method NMT that enable the ability of translation non-corpus language of the model.
  Firat et al.2016b propose an approach which delivers the multi-way,multilingual NMT model proposed by Firat et al.2016a to zero-resource translation.
  The authors' approach,however,also in this case the need of separate encoders and decoders for every language pair significantly increases the complexity of the model.

  
## Transformer.
  * The transformer architecture (Vaswani et al. 2017) works by relying on a selfattention mechanism, removing all the recurrent operations that are found in the RNN
case (Vaswani et al. 2017). In other words, the attention mechanism is re-purposed to
also compute the latent space representation of both the encoder and the decoder. The
right-hand side of Figure 3 depicts a simple one-layer encoder based on self-attention.
Notice that, in absence of recurrence, a positional-encoding is added to the input and
output embeddings. Similarly, as the time-step in RNN, the positional information
provides the transformer network with the order of input and output sequences. In
our work we use absolute positional encoding but, very recently, the use of relative
positional information has been shown to improve the network performance (Shaw,
Uszkoreit, and Vaswani 2018).
Overall, the transformer is organized as a stack of encoder-decoder networks that
works in an auto-regressive way, using the previously generated symbol as input for the
next prediction. Both the decoder and encoder can be composed of uniform layers, each
built of sub-layers, i.e., a multi-head self-attention sub-layer and a position-wise feedforward network sub-layer. Specifically for the decoder, an extra multi-head attentional
layer is added to attend to the output states of the encoder. Multi-head attention layers
enable the use of multiple attention functions with a computational cost similar to
utilizing a single attention.

## pivot-method NMT.
  * Cheng et al.
(2016a) propose a pivot-based method for zero resource NMT: it first translates the source language to a pivot language, which is then translated
to the target language.But this method has two problems, the computational complexity and error propagation.

# Background

The strength of NMT lies in its ability to learn directly,in a end to end fashion,the mapping from input text to associated output text(@sutskever2014sequence).
Lex $ \mathbf{x} $ be a source language sentence and $ \mathbf{y} $ be a target-language sentence.NMT directly estimates the conditional probability of the target variable $\mathbf{y}$
$ P\left(\mathbf{y} | \mathbf{x} ; {\boldsymbol{\theta}}_{x \rightarrow y}\right)\label{x1}, $
where $\boldsymbol{\theta}_{x \rightarrow y}$ is a set of model parameters.Given a source-target parallel corpus $D_{x, y}$ the model parameters can be obtained by maximum likelihood estimation(MLE) of $\eqref{#eq-x2}$.

~ Equation { #eq-x2 }
\hat{\boldsymbol{\theta}}_{x \rightarrow y}=\underset{\boldsymbol{\theta}_{x \rightarrow y}}{\operatorname{argmin}}\left\{- \sum_{(\mathbf{x}, \mathbf{y}) \in D_{x, y}} \log P\left(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}_{x \rightarrow y}\right)\right\}.
~


Although neural network-based models have shown remarkable results in the field of machine translation,but NMT still works poorly when the available number of training examples is limited(@koehn2003statistical).
zero-shot NMT is sensitive to training conditions, and the translation quality usually lags behind the pivot-based methods which use a shared language as a bridge for translation(@sestorain2018zero,@arivazhagan2019missing).
@cheng2016neural approaches to zero-resource NMT by using pivot-based NMT.Let $ \mathbf{x} $ , $ \mathbf{y} $,and $ \mathbf{z} $ denote source,target,and pivot languages. Then, the training objective of pivot-based NMT can be written as

~ Equation { #eq-x3 }
\begin{aligned} & P\left(\mathbf{y} | \mathbf{x} ; \boldsymbol{\theta}_{x \rightarrow z}, \boldsymbol{\theta}_{z \rightarrow y}\right) \\ =& \sum_{\mathbf{z}} P\left(\mathbf{z} | \mathbf{x} ; \boldsymbol{\theta}_{x \rightarrow z}\right) P\left(\mathbf{y} | \mathbf{z} ; \boldsymbol{\theta}_{z \rightarrow y}\right) \end{aligned}.
~


Given a source-pivot parallel corpus $D_{x,z}$ and the pivot-target parallel corpus $D_{z,y}$ the model parameter can be obtained separately by using MLE.

~ Equation { #eq-x4 }
\hat{\boldsymbol{\theta}}_{x \rightarrow z}=\underset{\theta_{x \rightarrow z}}{\operatorname{argmin}}\left\{- \sum_{\langle\mathbf{x}, \mathbf{z}) \in D_{x, z}} \log P\left(\mathbf{z} | \mathbf{x} ; \boldsymbol{\theta}_{x \rightarrow z}\right)\right\},
~

~ Equation { #eq-x5 }
\hat{\boldsymbol{\theta}}_{z \rightarrow y}=\underset{\theta_{z \rightarrow y}}{\operatorname{argmin}}\left\{- \sum_{\langle\mathbf{z}, \mathbf{y}\rangle \in D_{z, y}} \log P\left(\mathbf{y} | \mathbf{z} ; \boldsymbol{\theta}_{z \rightarrow y}\right)\right\}.
~


Therefore, it is necessary to explore methods to directly model source-to-target translation without parallel corpora available.
@chen2017teachers' approach overcomes the shortcomings of separation training. With the help of knowledge distillation, the pre-trained teacher model will not participate in the secondary decoding.
In fact,the student model trained that trained on the source-pivot corpus can obtain the target sentences in once decoding.

# Zero-shot in pivot-based NMT



The above pivot-based NMT requires a second decoding to obtain the target sentence.
The error generated in the first step will continue.
# Content

A definition of $e$ is shown in Equation [#euler]:

~ Equation { #euler }
e = \lim_{n\to\infty} \left( 1 + \frac{1}{n} \right)^n
~

Let's program some JavaScript:
``` javascript
function hello() {
  return "hello world!"
}
```

## There and back again

It had a perfectly round door like a porthole, painted green, with a
shiny yellow brass knob in the exact middle. The door opened on to a
tube-shaped hall like a tunnel: a very comfortable tunnel without smoke,
with paneled walls, and floors tiled and carpeted, provided with
polished chairs, and lots and lots of pegs for hats and coats; the hobbit
was fond of visitors. The tunnel wound on and on, going fairly but not
quite straight into the side of the hill -- The Hill, as all the people for
many miles round called it -- and many little round doors opened out of it,
first on one side and then on another. No going upstairs for the hobbit:
bedrooms, bathrooms, cellars, pantries (lots of these), wardrobes (he had
whole rooms devoted to clothes), kitchens, dining-rooms, all were on the
same floor, and indeed on the same passage. The best rooms were all on
the left-hand side (going in), for these were the only ones to have
windows, deep-set round windows looking over his garden, and meadows
beyond, sloping down to the river.


~ Lemma { #lemma-test; caption:"A __lemma__ caption" }
There he lay, a vast red-golden dragon, fast asleep; thrumming came from
his jaws and nostrils, and wisps of smoke, but his fires were low in
slumber
~

~ Proof { caption:"Of Lemma [#lemma-test]" }
Roads go ever ever on.
~

~ Todo 
Finish the proof
~

## The dinner

And suddenly first one and then another began to sing as they played,
deep-throated singing of the dwarves in the deep places of their ancient
homes; and this is like a fragment of their song, if it can be like their
song without their music. \[...\] As they sang the hobbit felt the love
of beautiful things made by hands and by cunning and by magic moving
through him, a fierce and jealous love, the desire of the hearts of
dwarves. Then something Tookish woke up inside him, and he wished to go
and see the great mountains, and hear the pine-trees and the waterfalls,
and explore the caves, and wear a sword instead of a walking-stick. He
looked out of the window. The stars were out in a dark sky above the
trees. He thought of the jewels of the dwarves shining in dark caverns.
Suddenly in the wood beyond The Water a flame leapt up -- probably
somebody lighting a wood-fire -- and he thought of plundering dragons
settling on his quiet Hill and kindling it all to flames. He shuddered;
and very quickly he was plain Mr. Baggins of Bag-End, Under-Hill, again.
He got up trembling.

## A turn of events

"Halt!" cried Gandalf, who appeared suddenly, and stood alone, with arms
uplifted, between the advancing dwarves and the ranks awaiting them.
"Halt!" he called in a voice like thunder, and his staff blazed forth
with a flash like the lightning. "Dread has come upon you all! Alas! it
has come more swiftly than I guessed."

    for i:=maxint to 0 do
    begin 
        j:=square(root(i));
    end;

"If you mean you think it is my job to go into the secret passage first, O
Thorin Thrain's son Oakenshield, may your beard grow ever longer," he
said crossly, "say so at once and have done!"

## A description

I suppose hobbits need some description nowadays, since they have become
rare and shy of the Big People, as they call us. They are (or were) a
little people, about half our height, and smaller than the bearded
Dwarves. Hobbits have no beards. There is little or no magic about them,
except the ordinary everyday sort which helps them to disappear quietly
and quickly when large stupid folk like you and me come blundering along,
making a noise like elephants which they can hear a mile off. They are
inclined to be fat in the stomach; they dress in bright colors (chiefly
green and yellow);

~ Lemma { caption:"Another lemma" }
There is little or no magic about them, except the ordinary everyday sort
which helps them to disappear quietly and quickly when large stupid folk
like you and me come blundering along, making a noise like elephants
which they can hear a mile off.
~


### Acknowledgments {-}

I would like to thank ...

[BIB]

&pagebreak;

# An appendix { @h1:"A" }

"All the same, I should like it all plain and clear," said he
obstinately, putting on his business manner (usually reserved for people
who tried to borrow money off him), and doing his best to appear wise and
prudent and professional and live up to Gandalf's recommendation. "Also I
should like to know about risks, out-of-pocket expenses, time required
and remuneration, and so forth" -- by which he meant: "What am I going to
get out of it? and am I going to come back alive?"

# Conclusion

Really fun to write Markdown :-)
