Title         : My Academic Paper

Author        : Yuzhe Liu
Affiliation   : XJ
Email         : dotaofll@163.com

Author        : My co-author
Affiliation   : Other institute
Email         : other@bar.com

Bibliography  : example.bib
Doc class     : [reprint,nocopyrightspace]style/sigplanconf.cls

[TITLE]

~ Abstract
There is more in you of good than you know, child of the kindly West.
Some courage and some wisdom, blended in measure. If more of us valued
food and cheer and song above hoarded gold, it would be a merrier world.
But sad or merry, I must leave it now. Farewell!
~

~ TexRaw
% any commands necessary for your particular style
\category{D.2.5}{Software Engineering}{Testing and Debugging}[symbolic execution]
\terms{Algorithms, Experimentation}
\keywords{Games for learning, white box testing}
\cite{@johnson2017google}
~

# Introduction     { #sec-intro }

Let's start with a sub section.

# Related Work


## zero-shot learning.
 * try to explain the NMT has attract more people attention.
  The NMT system consists of an encoder and decoder, which jointly train the maximum likelihood probability in an end-to-end manner.
  With the help of long-term and short-term memory neural networks, threshold loop units, attention mechanisms, etc., NMT has achieved good results in many translation tasks, especially the latter.
  Among the different variants, two are the most concerned, one is recurrent NMT and the other is transformer NMT models.
  
 * google's team propose a multilingual method NMT that enable the ability of translation non-corpus language of the model.
  Firat et al.2016b propose an approach which delivers the multi-way,multilingual NMT model proposed by Firat et al.2016a to zero-resource translation.
  The authors' approach,however,also in this case the need of separate encoders and decoders for every language pair significantly increases the complexity of the model.

  
## Transformer.
  * The transformer architecture (Vaswani et al. 2017) works by relying on a selfattention mechanism, removing all the recurrent operations that are found in the RNN
case (Vaswani et al. 2017). In other words, the attention mechanism is re-purposed to
also compute the latent space representation of both the encoder and the decoder. The
right-hand side of Figure 3 depicts a simple one-layer encoder based on self-attention.
Notice that, in absence of recurrence, a positional-encoding is added to the input and
output embeddings. Similarly, as the time-step in RNN, the positional information
provides the transformer network with the order of input and output sequences. In
our work we use absolute positional encoding but, very recently, the use of relative
positional information has been shown to improve the network performance (Shaw,
Uszkoreit, and Vaswani 2018).
Overall, the transformer is organized as a stack of encoder-decoder networks that
works in an auto-regressive way, using the previously generated symbol as input for the
next prediction. Both the decoder and encoder can be composed of uniform layers, each
built of sub-layers, i.e., a multi-head self-attention sub-layer and a position-wise feedforward network sub-layer. Specifically for the decoder, an extra multi-head attentional
layer is added to attend to the output states of the encoder. Multi-head attention layers
enable the use of multiple attention functions with a computational cost similar to
utilizing a single attention.

## pivot-method NMT.
  * Cheng et al.
(2016a) propose a pivot-based method for zero resource NMT: it first translates the source language to a pivot language, which is then translated
to the target language.But this method has two problems, the computational complexity and error propagation.

## A short overview

Figure [#fig-butterfly] in Section [#sec-intro] shows a monarch
butterfly. Note that you can drag&drop images into the editor pane to
include them in the document, and similarly with bibtex files, latex
style files, etc.

~ Figure { #fig-butterfly; caption:"A Monarch butterfly (use the `.wide` class for wide figure)."; page-align:top}
![butterfly]
~

[butterfly]: images/butterfly.png "butterfly"  { width:4em }

Our contributions are:

* A figure of a _butterfly_;
* Some **mathematics**;
* And some source code;
* And references to Tex books [@Knuth:TeX;@Lamport:LaTeX;@Fberg04] and others [@Grandstrand] @johnson2017google

# Content

A definition of $e$ is shown in Equation [#euler]:

~ Equation { #euler }
e = \lim_{n\to\infty} \left( 1 + \frac{1}{n} \right)^n
~

Let's program some JavaScript:
``` javascript
function hello() {
  return "hello world!"
}
```

## There and back again

It had a perfectly round door like a porthole, painted green, with a
shiny yellow brass knob in the exact middle. The door opened on to a
tube-shaped hall like a tunnel: a very comfortable tunnel without smoke,
with paneled walls, and floors tiled and carpeted, provided with
polished chairs, and lots and lots of pegs for hats and coats; the hobbit
was fond of visitors. The tunnel wound on and on, going fairly but not
quite straight into the side of the hill -- The Hill, as all the people for
many miles round called it -- and many little round doors opened out of it,
first on one side and then on another. No going upstairs for the hobbit:
bedrooms, bathrooms, cellars, pantries (lots of these), wardrobes (he had
whole rooms devoted to clothes), kitchens, dining-rooms, all were on the
same floor, and indeed on the same passage. The best rooms were all on
the left-hand side (going in), for these were the only ones to have
windows, deep-set round windows looking over his garden, and meadows
beyond, sloping down to the river.


~ Lemma { #lemma-test; caption:"A __lemma__ caption" }
There he lay, a vast red-golden dragon, fast asleep; thrumming came from
his jaws and nostrils, and wisps of smoke, but his fires were low in
slumber
~

~ Proof { caption:"Of Lemma [#lemma-test]" }
Roads go ever ever on.
~

~ Todo 
Finish the proof
~

## The dinner

And suddenly first one and then another began to sing as they played,
deep-throated singing of the dwarves in the deep places of their ancient
homes; and this is like a fragment of their song, if it can be like their
song without their music. \[...\] As they sang the hobbit felt the love
of beautiful things made by hands and by cunning and by magic moving
through him, a fierce and jealous love, the desire of the hearts of
dwarves. Then something Tookish woke up inside him, and he wished to go
and see the great mountains, and hear the pine-trees and the waterfalls,
and explore the caves, and wear a sword instead of a walking-stick. He
looked out of the window. The stars were out in a dark sky above the
trees. He thought of the jewels of the dwarves shining in dark caverns.
Suddenly in the wood beyond The Water a flame leapt up -- probably
somebody lighting a wood-fire -- and he thought of plundering dragons
settling on his quiet Hill and kindling it all to flames. He shuddered;
and very quickly he was plain Mr. Baggins of Bag-End, Under-Hill, again.
He got up trembling.

## A turn of events

"Halt!" cried Gandalf, who appeared suddenly, and stood alone, with arms
uplifted, between the advancing dwarves and the ranks awaiting them.
"Halt!" he called in a voice like thunder, and his staff blazed forth
with a flash like the lightning. "Dread has come upon you all! Alas! it
has come more swiftly than I guessed."

    for i:=maxint to 0 do
    begin 
        j:=square(root(i));
    end;

"If you mean you think it is my job to go into the secret passage first, O
Thorin Thrain's son Oakenshield, may your beard grow ever longer," he
said crossly, "say so at once and have done!"

## A description

I suppose hobbits need some description nowadays, since they have become
rare and shy of the Big People, as they call us. They are (or were) a
little people, about half our height, and smaller than the bearded
Dwarves. Hobbits have no beards. There is little or no magic about them,
except the ordinary everyday sort which helps them to disappear quietly
and quickly when large stupid folk like you and me come blundering along,
making a noise like elephants which they can hear a mile off. They are
inclined to be fat in the stomach; they dress in bright colors (chiefly
green and yellow);

~ Lemma { caption:"Another lemma" }
There is little or no magic about them, except the ordinary everyday sort
which helps them to disappear quietly and quickly when large stupid folk
like you and me come blundering along, making a noise like elephants
which they can hear a mile off.
~


### Acknowledgments {-}

I would like to thank ...

[BIB]

&pagebreak;

# An appendix { @h1:"A" }

"All the same, I should like it all plain and clear," said he
obstinately, putting on his business manner (usually reserved for people
who tried to borrow money off him), and doing his best to appear wise and
prudent and professional and live up to Gandalf's recommendation. "Also I
should like to know about risks, out-of-pocket expenses, time required
and remuneration, and so forth" -- by which he meant: "What am I going to
get out of it? and am I going to come back alive?"

# Conclusion

Really fun to write Markdown :-)
